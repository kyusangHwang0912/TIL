{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8장 모델평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 01. 분류 성능 평가 지표\n",
    "- 오차행렬(confusion matrix)\n",
    "- 정확도(accuracy)\n",
    "    - 예측결과와 실제값이 동일한 건수 / 전체데이터 수\n",
    "- 정밀도와 재현율(트레이드 오프 관계)\n",
    "    - 정밀도(precision)\n",
    "        - 예측을 Positive로 한 대상 중에 예측과 실제 값이 Positive로 일치한 데이터의 비율\n",
    "    - 재현율 = 민감도\n",
    "        - 실제 값이 Positive인 대상 중에 예측과 실제 값이 Positive로 일치한 데이터의 비율\n",
    "    - 재현율이 중요지표인 경우는 실제 Positive 양성 데이터를 Negative로 잘못 판단하게 되면 업무상 큰 영향이 발생하는 경우 (암판단, 금융사기)\n",
    "    - 정밀도가 중요지표인 경우는 실제 Negative 음성인 데이터 예측을 Positive 양성으로 잘못 판단하게 되면 큰 영향이 발생하는 경우 (스팸)        \n",
    "- F1 score\n",
    "    - 정밀도와 재현율을 겹합한 지표\n",
    "- ROC 곡선과 AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion_matrix로 모델평가지표 구하기\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def model_evaluation(label, predict):\n",
    "    cf_matrix = confusion_matrix(label, predict)\n",
    "    Accuracy = (cf_matrix[0][0] + cf_matrix[1][1]) / sum(sum(cf_matrix))\n",
    "    Precision = cf_matrix[1][1] / (cf_matrix[1][1] + cf_matrix[0][1])\n",
    "    Recall = cf_matrix[1][1] / (cf_matrix[1][1] + cf_matrix[1][0])\n",
    "    F1_Score = (2 * Recall * Precision) / (Recall + Precision)\n",
    "    print(\"Model_Evaluation with Label:1\")\n",
    "    print(\"Accuracy: \", Accuracy)\n",
    "    print(\"Precision: \", Precision)\n",
    "    print(\"Recall: \", Recall)\n",
    "    print(\"F1-Score: \", F1_Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각각 따로 구할 수 있음\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC 커브와 AUC 구하기\n",
    "from sklearn import metrics\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, pred_y, pos_label=1)\n",
    "\n",
    "plt.plot(fpr,tpr)\n",
    "\n",
    "auc = np.trapz(tpr,fpr)\n",
    "print('AUC:', auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 02. 회귀 성능 평가 지표\n",
    "- MAE(mean absolute error)\n",
    "    - 실제 값과 예측값의 차이를 절대값으로 변환해 평균한 것\n",
    "- MSE(mean squared error)\n",
    "    - 실제 값과 예측값의 차이를 제곱해 평균한 것\n",
    "- RMSE(root mean squared error)\n",
    "    - mse는 오류의 제곱을 구하므로 실제 오류 평균보다 더 커지는 특성이 있음으로 mse에 루트를 씌운 것\n",
    "- R^2(r-squared)\n",
    "    - 분산 기반으로 예측 성능을 평가한다. 실제 값의 분산 대비 예측값의 분산 비율을 지표로 하며, 1에 가까울수록 예측 정확도가 높다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# MSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# R^2\n",
    "from sklearn.metrics import r2_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### @ scroing 함수 적용 값\n",
    "- MAE\n",
    "    - 'neg_mean_abolute_error'\n",
    "- MSE\n",
    "    - 'neg_mean_squared_error'\n",
    "- r^2\n",
    "    - 'r2'\n",
    "- **'neg'라는 접두어는 음수 값을 가진다는 의미이다. 따라서 주의해야 한다. -1 을 곱한 값들이기 때문에 다시 -1곱해줘야 원래 값인 것!**\n",
    "\n",
    "\n",
    "- 릿지,라쏘 쪽 참고 예시\n",
    "```python\n",
    "for alpha in alphas :\n",
    "    elasticnet = ElasticNet(alpha = alpha)\n",
    "    \n",
    "    neg_mse_scores = cross_val_score(elasticnet, X_data, y_target, scoring='neg_mean_squared_error', cv=5)\n",
    "    rmse_scores = np.sqrt(-1 * neg_mse_scores)\n",
    "    avg_rmse = np.mean(rmse_scores)\n",
    "    print('alpha={0}일때, 5 foldas의 평균 RMSE={1:4f}'.format(alpha,avg_rmse))\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
